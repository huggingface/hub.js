/**
 * Inference code generated from the JSON schema spec in ./spec
 *
 * Using src/scripts/inference-codegen
 */

/**
 * Chat Completion Input
 */
export interface ChatCompletionInput {
	/**
	 * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
	 * frequency in the text so far,
	 * decreasing the model's likelihood to repeat the same line verbatim.
	 */
	frequency_penalty?: number;
	/**
	 * UNUSED
	 * Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON
	 * object that maps tokens
	 * (specified by their token ID in the tokenizer) to an associated bias value from -100 to
	 * 100. Mathematically,
	 * the bias is added to the logits generated by the model prior to sampling. The exact
	 * effect will vary per model,
	 * but values between -1 and 1 should decrease or increase likelihood of selection; values
	 * like -100 or 100 should
	 * result in a ban or exclusive selection of the relevant token.
	 */
	logit_bias?: number[];
	/**
	 * Whether to return log probabilities of the output tokens or not. If true, returns the log
	 * probabilities of each
	 * output token returned in the content of message.
	 */
	logprobs?: boolean;
	/**
	 * The maximum number of tokens that can be generated in the chat completion.
	 */
	max_tokens?: number;
	/**
	 * A list of messages comprising the conversation so far.
	 */
	messages: MessageElement[];
	/**
	 * [UNUSED] ID of the model to use. See the model endpoint compatibility table for details
	 * on which models work with the Chat API.
	 */
	model: string;
	/**
	 * UNUSED
	 * How many chat completion choices to generate for each input message. Note that you will
	 * be charged based on the
	 * number of generated tokens across all of the choices. Keep n as 1 to minimize costs.
	 */
	n?: number;
	/**
	 * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they
	 * appear in the text so far,
	 * increasing the model's likelihood to talk about new topics
	 */
	presence_penalty?: number;
	seed?: number;
	/**
	 * Up to 4 sequences where the API will stop generating further tokens.
	 */
	stop?: string[];
	stream?: boolean;
	/**
	 * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the
	 * output more random, while
	 * lower values like 0.2 will make it more focused and deterministic.
	 *
	 * We generally recommend altering this or `top_p` but not both.
	 */
	temperature?: number;
	tool_choice?: ToolChoice;
	/**
	 * A prompt to be appended before the tools
	 */
	tool_prompt?: string;
	/**
	 * A list of tools the model may call. Currently, only functions are supported as a tool.
	 * Use this to provide a list of
	 * functions the model may generate JSON inputs for.
	 */
	tools?: ToolElement[];
	/**
	 * An integer between 0 and 5 specifying the number of most likely tokens to return at each
	 * token position, each with
	 * an associated log probability. logprobs must be set to true if this parameter is used.
	 */
	top_logprobs?: number;
	/**
	 * An alternative to sampling with temperature, called nucleus sampling, where the model
	 * considers the results of the
	 * tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10%
	 * probability mass are considered.
	 */
	top_p?: number;
	[property: string]: unknown;
}

export interface MessageElement {
	content?: string;
	name?: string;
	role: string;
	tool_calls?: ToolCallElement[];
	[property: string]: unknown;
}

export interface ToolCallElement {
	function: ToolFunction;
	id: number;
	type: string;
	[property: string]: unknown;
}

export interface ToolFunction {
	arguments: unknown;
	description?: string;
	name: string;
	[property: string]: unknown;
}

export type ToolChoice = "OneOf" | ToolChoiceObject;

export interface ToolChoiceObject {
	FunctionName: string;
	[property: string]: unknown;
}

export interface ToolElement {
	function: ToolFunction;
	type: string;
	[property: string]: unknown;
}

/**
 * Chat Completion Output
 */
export interface ChatCompletionOutput {
	choices: ChoiceElement[];
	created: number;
	id: string;
	model: string;
	object: string;
	system_fingerprint: string;
	usage: Usage;
	[property: string]: unknown;
}

export interface ChoiceElement {
	finish_reason: string;
	index: number;
	logprobs?: PurpleLogprobs;
	message: Message;
	[property: string]: unknown;
}

export interface PurpleLogprobs {
	content: ContentElement[];
	[property: string]: unknown;
}

export interface ContentElement {
	logprob: number;
	token: string;
	top_logprobs: TopLogprobElement[];
	[property: string]: unknown;
}

export interface TopLogprobElement {
	logprob: number;
	token: string;
	[property: string]: unknown;
}

export interface Message {
	content?: string;
	name?: string;
	role: string;
	tool_calls?: ToolCallObject[];
	[property: string]: unknown;
}

export interface ToolCallObject {
	function: PurpleFunction;
	id: number;
	type: string;
	[property: string]: unknown;
}

export interface PurpleFunction {
	arguments: unknown;
	description?: string;
	name: string;
	[property: string]: unknown;
}

export interface Usage {
	completion_tokens: number;
	prompt_tokens: number;
	total_tokens: number;
	[property: string]: unknown;
}

/**
 * Chat Completion Stream Output
 */
export interface ChatCompletionStreamOutput {
	choices: ChoiceObject[];
	created: number;
	id: string;
	model: string;
	object: string;
	system_fingerprint: string;
	[property: string]: unknown;
}

export interface ChoiceObject {
	delta: Delta;
	finish_reason?: string;
	index: number;
	logprobs?: FluffyLogprobs;
	[property: string]: unknown;
}

export interface Delta {
	content?: string;
	role: string;
	tool_calls?: ToolCalls;
	[property: string]: unknown;
}

export interface ToolCalls {
	function: ToolCallsFunction;
	id: string;
	index: number;
	type: string;
	[property: string]: unknown;
}

export interface ToolCallsFunction {
	arguments: string;
	name?: string;
	[property: string]: unknown;
}

export interface FluffyLogprobs {
	content: ContentObject[];
	[property: string]: unknown;
}

export interface ContentObject {
	logprob: number;
	token: string;
	top_logprobs: TopLogprobObject[];
	[property: string]: unknown;
}

export interface TopLogprobObject {
	logprob: number;
	token: string;
	[property: string]: unknown;
}
